{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "load project from github"
      ],
      "metadata": {
        "id": "RNGqXGrJZfEz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ql8MQaNlZE6G"
      },
      "outputs": [],
      "source": [
        "# Load DANN-DL github, populate necessary repositories\n",
        "ls = !ls\n",
        "if \"CoRL_OODWorkshop_DANN-DL\" in ls[0]:\n",
        "  None\n",
        "else:\n",
        "  !git clone https://github.com/StanfordASL/CoRL_OODWorkshop_DANN-DL.git\n",
        "  !cd CoRL_OODWorkshop_DANN-DL/Modules && git clone https://github.com/tadeephuy/GradientReversal.git\n",
        "  !cd CoRL_OODWorkshop_DANN-DL/Modules && git clone https://github.com/StanfordASL/scod-module\n",
        "  !cd CoRL_OODWorkshop_DANN-DL/Modules && mv ./scod-module ./scod_module"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "library import"
      ],
      "metadata": {
        "id": "Ji8luhd9aB2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import os\n",
        "import sys\n",
        "\n",
        "pwd = !pwd\n",
        "project_dir = os.path.join(pwd[0], \"CoRL_OODWorkshop_DANN-DL\")\n",
        "sys.path.append(project_dir)\n",
        "\n",
        "from Modules.DANN.DANN import DANN, DA_parameter\n",
        "from Modules.CNN.CNN_MNIST import CNN_MNIST\n",
        "from Modules.DANN.DANN_train import DANN_train\n",
        "from Modules.DANN.DANN_eval import DANN_eval\n",
        "from get_device import get_device\n",
        "\n",
        "from Modules.scod_module.scod.scod import SCOD as SCOD_CNN\n",
        "from Modules.scod_module.scod.scod import OodDetector\n",
        "from Modules.scod_module.scod.distributions import CategoricalLogitLayer\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision.datasets import MNIST, ImageFolder\n",
        "from torchvision import transforms\n",
        "from tqdm import trange"
      ],
      "metadata": {
        "id": "I0uXy3XZaFtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "meta data"
      ],
      "metadata": {
        "id": "zQjW6mA-aGkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "epochs = 100\n",
        "batch_size = 64\n",
        "lr = 1e-4\n",
        "domain_loss_weight = 0.1\n",
        "\n",
        "# Training set\n",
        "source_train_size = 11000\n",
        "target_train_size = 0 # no target data at initial deployment -- we don't know what we'll see\n",
        "image_size = 28\n",
        "\n",
        "# Deployment\n",
        "num_episodes = 10\n",
        "images_per_episode = 75\n",
        "percent_OOD = 0.2 # 0.05\n",
        "scod_percentile = 0.95 # OOD threshold (on source)\n",
        "\n",
        "print(\"Episodes: \", num_episodes)\n",
        "print(\"Percent data is OOD: \", percent_OOD, \"\\n\")\n",
        "\n",
        "# Run chacterization\n",
        "run_name = \"local_kl\" # SCOD method\n",
        "assert run_name == \"local_kl\" or run_name == \"entropy\" or run_name == \"var\""
      ],
      "metadata": {
        "id": "uMbJGMM0aHM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load datasets"
      ],
      "metadata": {
        "id": "eWvbehZabHvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "mnistm_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.CenterCrop(image_size)\n",
        "])\n",
        "\n",
        "_train_source = MNIST(root = os.path.join(project_dir, 'MNIST/train'), train = True, download = True, transform = mnist_transform)\n",
        "_train_target = ImageFolder(os.path.join(project_dir, 'MNIST-M/training'), transform = mnistm_transform)\n",
        "\n",
        "train_source, _deploy_source, _ = torch.utils.data.random_split(_train_source, lengths = [source_train_size, int(images_per_episode*(1-percent_OOD))*num_episodes, len(_train_source) - source_train_size - int(images_per_episode*(1-percent_OOD))*num_episodes])\n",
        "train_target, _deploy_target, _ = torch.utils.data.random_split(_train_target, lengths = [target_train_size, int(images_per_episode*percent_OOD)*num_episodes, len(_train_target) - target_train_size - int(images_per_episode*percent_OOD)*num_episodes])\n",
        "\n",
        "deploy_sources = torch.utils.data.random_split(_deploy_source, lengths = [1/num_episodes]*num_episodes)\n",
        "deploy_targets = torch.utils.data.random_split(_deploy_target, lengths = [1/num_episodes]*num_episodes)\n",
        "\n",
        "test_source = MNIST(root = os.path.join(project_dir, \"MNIST/test\"), train = False, download = True, transform = mnist_transform)\n",
        "test_target = ImageFolder(os.path.join(project_dir, \"MNIST-M/testing\"), transform = mnistm_transform)\n",
        "\n",
        "print(\"Source training set size: \", len(train_source))\n",
        "print(\"Source images available for deployment: \", int(images_per_episode*(1-percent_OOD))*num_episodes)\n",
        "print(\"Source images per episode: \", len(deploy_sources[0]))\n",
        "\n",
        "print(\"Target training set size: \", len(train_target))\n",
        "print(\"Target images available for deployment: \", int(images_per_episode*percent_OOD)*num_episodes)\n",
        "print(\"Target images per episode: \", len(deploy_targets[0]))\n",
        "\n",
        "print(\"Source test set size: \", len(test_source))\n",
        "print(\"Target test set size: \", len(test_target), '\\n')"
      ],
      "metadata": {
        "id": "9-dnUcMfbJO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "device definition"
      ],
      "metadata": {
        "id": "lqUTh_FwbMj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = get_device()"
      ],
      "metadata": {
        "id": "iWM_GBTjbPDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "incorporate scod object"
      ],
      "metadata": {
        "id": "kyGi1btQdSQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_scod(run_name, model, train_source):\n",
        "  args = {\n",
        "    'num_eigs': 100,\n",
        "    'num_samples': 304,\n",
        "    'sketch_type': 'srft'\n",
        "  }\n",
        "\n",
        "  # only use SCOD from CNN\n",
        "  scod_model = SCOD_CNN(model, args = args)\n",
        "\n",
        "  dist_layer = CategoricalLogitLayer()\n",
        "  scod_model.process_dataset(train_source, dist_layer)\n",
        "  ood_detector = OodDetector(scod_model, dist_layer, run_name)\n",
        "\n",
        "  return ood_detector\n",
        "\n",
        "def collate_fn(batch):\n",
        "  return torch.stack([x[0].expand(3, -1, -1) for x in batch]), torch.hstack([torch.tensor([x[1]]) for x in batch])\n",
        "\n",
        "def perform_detection(ood_detector, deploy_source, deploy_target, source, percentile):\n",
        "  batch_size = 64\n",
        "  deploy_source_dataloader = torch.utils.data.DataLoader(deploy_source, batch_size = batch_size)\n",
        "  deploy_target_dataloader = torch.utils.data.DataLoader(deploy_target, batch_size = batch_size)\n",
        "  source_dataloader = torch.utils.data.DataLoader(source, batch_size = batch_size, collate_fn = collate_fn)\n",
        "\n",
        "  scod_source = torch.cat([ood_detector(batch[0].to(device)).detach().cpu() for batch in deploy_source_dataloader])\n",
        "  scod_target = torch.cat([ood_detector(batch[0].to(device)).detach().cpu() for batch in deploy_target_dataloader])\n",
        "  scod_train_source = torch.cat([ood_detector(batch[0].to(device)).detach().cpu() for batch in source_dataloader])\n",
        "\n",
        "  ood_source = torch.utils.data.Subset(deploy_source, torch.where(scod_source > torch.quantile(scod_train_source, q = percentile))[0])\n",
        "  ood_target = torch.utils.data.Subset(deploy_target, torch.where(scod_target > torch.quantile(scod_train_source, q = percentile))[0])\n",
        "\n",
        "  return ood_source, ood_target"
      ],
      "metadata": {
        "id": "qe6gNeS6dU7Z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "simulate deployment"
      ],
      "metadata": {
        "id": "qMbdmMJsbPdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First episode CNN deployment\n",
        "episode = 0\n",
        "model = CNN_MNIST()\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 100)\n",
        "ce_loss = torch.nn.CrossEntropyLoss(reduction = \"sum\")\n",
        "\n",
        "# train CNN classifier\n",
        "mnist_dataloader = torch.utils.data.DataLoader(train_source, batch_size = batch_size, shuffle = True, pin_memory = True)\n",
        "writer = SummaryWriter(\"./CoRL_OODWorkshop_DANN-DL/Tensorboard/\" + run_name + \"/episode\" + str(episode))\n",
        "\n",
        "for epoch in trange(100):\n",
        "  epoch_loss = 0.\n",
        "  for ip, op in mnist_dataloader:\n",
        "      ip, op = ip.to(device), op.to(device) # transfer ip/op to GPU, if applicable\n",
        "      pred = model(ip)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss = ce_loss(pred, op)\n",
        "      mean_loss = loss / ip.shape[0]\n",
        "      mean_loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      epoch_loss += loss.item()\n",
        "  epoch_loss /= len(train_source)\n",
        "  scheduler.step()\n",
        "  writer.add_scalar(\"mean_epoch_loss\", epoch_loss, epoch)\n",
        "\n",
        "torch.save(model, \"./CoRL_OODWorkshop_DANN-DL/Tensorboard/\" + run_name + \"/episode\" + str(episode) + \"/episode\" + str(episode) + \"_model.pth\")\n",
        "writer.flush()\n",
        "\n",
        "# Evaluate CNN\n",
        "softmax = torch.nn.Softmax(dim = -1)\n",
        "test_mnist_dataloader = torch.utils.data.DataLoader(test_source, batch_size = batch_size)\n",
        "test_mnistm_dataloader = torch.utils.data.DataLoader(test_target, batch_size = batch_size)\n",
        "\n",
        "mnist_correct = 0\n",
        "mnistm_correct = 0\n",
        "\n",
        "for ip, op in test_mnist_dataloader:\n",
        "    ip, op = ip.to(device), op.to(device)\n",
        "\n",
        "    pred_logits = model(ip)\n",
        "    pred_softmax = softmax(pred_logits)\n",
        "    pred = torch.argmax(pred_softmax, dim = -1)\n",
        "\n",
        "    mnist_correct += len(torch.where(pred == op)[0])\n",
        "\n",
        "for ip, op in test_mnistm_dataloader:\n",
        "    ip, op = ip.to(device), op.to(device)\n",
        "\n",
        "    pred_logits = model(ip)\n",
        "    pred_softmax = softmax(pred_logits)\n",
        "    pred = torch.argmax(pred_softmax, dim = -1)\n",
        "\n",
        "    mnistm_correct += len(torch.where(pred == op)[0])\n",
        "\n",
        "print(\"CNN MNIST Accuracy: \", mnist_correct / len(test_source))\n",
        "print(\"CNN MNIST-M Accuracy: \", mnistm_correct / len(test_target))\n",
        "\n",
        "writer.add_scalar(\"source_acc\", mnist_correct / len(test_source), episode)\n",
        "writer.add_scalar(\"target_acc\", mnistm_correct / len(test_target), episode)\n",
        "writer.add_scalar(\"source_training_size\", len(train_source), episode)\n",
        "writer.add_scalar(\"target_training_size\", len(train_target), episode)\n",
        "writer.flush()\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "MU7YBw0hnvQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- \" + run_name + \" ---\")\n",
        "\n",
        "ood_detector = build_scod(run_name, model, train_source)\n",
        "\n",
        "# Build DANN architecture after first deployment\n",
        "for episode in range(1, num_episodes+1):\n",
        "\n",
        "  # perform OOD detection with previous model\n",
        "  ood_source, ood_target = perform_detection(\n",
        "      ood_detector,\n",
        "      deploy_sources[episode-1],\n",
        "      deploy_targets[episode-1],\n",
        "      train_source,\n",
        "      scod_percentile)\n",
        "  if len(train_source) < len(train_target) + len(ood_source) + len(ood_target):\n",
        "    break # stop after target larger than source\n",
        "\n",
        "  dirpath = \"./CoRL_OODWorkshop_DANN-DL/Tensorboard/\" + run_name + \"/episode\" + str(episode)\n",
        "  writer = SummaryWriter(dirpath)\n",
        "\n",
        "  # train model\n",
        "  model, train_target, writer = DANN_train(\n",
        "      train_source,\n",
        "      train_target,\n",
        "      torch.utils.data.ConcatDataset((ood_source, ood_target)),\n",
        "      test_target,\n",
        "      (epochs, lr, batch_size, 0, domain_loss_weight),\n",
        "      (device, writer)\n",
        "    )\n",
        "  torch.save(model, dirpath + \"/episode\" + str(episode) + \"_model.pth\")\n",
        "\n",
        "  # evaluate model\n",
        "  source_acc, target_acc = DANN_eval(model, test_source, test_target, (batch_size, 0), device)\n",
        "  writer.add_scalar(\"source_acc\", source_acc, episode)\n",
        "  writer.add_scalar(\"target_acc\", target_acc, episode)\n",
        "\n",
        "  writer.add_scalar(\"source_training_size\", len(train_source), episode)\n",
        "  writer.add_scalar(\"target_training_size\", len(train_target), episode)\n",
        "\n",
        "  writer.add_scalar(\"false_positive_rate\", len(ood_source)/len(deploy_sources[episode-1]), episode) # store false positives\n",
        "  writer.add_scalar(\"true_positive_rate\", len(ood_target)/len(deploy_targets[episode-1]), episode) # store true negatives\n",
        "\n",
        "  writer.flush()\n",
        "  writer.close()\n",
        "\n",
        "  print(\"Episode \", episode)\n",
        "  print(\"\\tSource acc: \", source_acc)\n",
        "  print(\"\\tTarget acc: \", target_acc)\n",
        "  print(\"\\tTarget training size: \", len(train_target))\n",
        "  print(\"\\t\\tfalse positive rate: \", len(ood_source)/len(deploy_sources[episode-1]))\n",
        "  print(\"\\t\\ttrue positive rate: \", len(ood_target)/len(deploy_targets[episode-1]))"
      ],
      "metadata": {
        "id": "s3KQ8RWhbQzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r ./results.zip ./CoRL_OODWorkshop_DANN-DL/Tensorboard/local_kl/"
      ],
      "metadata": {
        "id": "urtix5njXZx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"./results.zip\")"
      ],
      "metadata": {
        "id": "-1ov8ogqWuuH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}